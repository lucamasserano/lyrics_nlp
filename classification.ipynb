{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from tfidf.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nbimporter\n",
    "from tfidf import LyricsTFIDF\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LyricsClassification CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class LyricsClassification():\n",
    "    \n",
    "    \n",
    "    def __init__(self, lyrics_df, genres=None, preprocessed_corpus=None):\n",
    "        \n",
    "        if genres is None:\n",
    "            self.lyrics_df = lyrics_df\n",
    "            self.genres=lyrics_df.genre.unique()\n",
    "        else:\n",
    "            self.lyrics_df = lyrics_df.loc[lyrics_df.genre.isin(genres), :]\n",
    "            self.genres=genres\n",
    "        \n",
    "        self.preprocessed_corpus = preprocessed_corpus\n",
    "        self.tfidf_model = LyricsTFIDF(lyrics_df, preprocessed_corpus)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def divide_dataset(self, lyrics_df_preprocessed, train_size=0.7, dev_size=0.15, test_size=0.15):\n",
    "        \n",
    "        # stratified sampling respects class proportions\n",
    "        train, dev_and_test = train_test_split(lyrics_df_preprocessed, \n",
    "                                               train_size=train_size, test_size=dev_size+test_size, \n",
    "                                               stratify=lyrics_df_preprocessed.genre)\n",
    "        \n",
    "        # repeat on dev_and_test to obtain separate dev and test sets\n",
    "        dev, test = train_test_split(dev_and_test, \n",
    "                                     train_size=dev_size/(dev_size+test_size), test_size=test_size/(dev_size+test_size), \n",
    "                                     stratify=dev_and_test.genre)\n",
    "        \n",
    "        self.train_set, self.dev_set, self.test_set = train, dev, test\n",
    "        \n",
    "        return train, dev, test\n",
    "        \n",
    "    \n",
    "    def encode_labels(self, train_set, dev_set, test_set, target_variable='genre'):\n",
    "\n",
    "        # transform labels into numbers\n",
    "        labels2numbers = LabelEncoder()\n",
    "\n",
    "        y_train = labels2numbers.fit_transform(train_set.loc[:, target_variable])\n",
    "        y_dev = labels2numbers.transform(dev_set.loc[:, target_variable])\n",
    "        y_test = labels2numbers.transform(test_set.loc[:, target_variable])\n",
    "        \n",
    "        self.y_train, self.y_dev, self.y_test = y_train, y_dev, y_test\n",
    "        self.label_encoder = labels2numbers\n",
    "        \n",
    "        return y_train, y_dev, y_test\n",
    "    \n",
    "    \n",
    "    def transform_input(self, train_set, dev_set, test_set, ngram_range=(1,3), add_year=False, **kwargs):\n",
    "        \n",
    "        # train --> fit transform\n",
    "        print(\"Transforming train ...\", flush=True)\n",
    "        tfidf_dataframe = self.tfidf_model.fit_transform(preprocessed_corpus=train_set.loc[:, \"lyrics\"], \n",
    "                                                         ngram_range=ngram_range, **kwargs)\n",
    "        self.train_tfidf_matrix = self.tfidf_model.tfidf_matrix\n",
    "        \n",
    "        # dev --> transform\n",
    "        print(\"Transforming dev ...\", flush=True)\n",
    "        dev_corpus = [\" \".join(lyrics) for lyrics in tqdm(dev_set.loc[:, \"lyrics\"])]\n",
    "        self.dev_tfidf_matrix = self.tfidf_model.tfidf_vectorizer.transform(dev_corpus)\n",
    "        \n",
    "        # test --> transform\n",
    "        print(\"Transforming test ...\", flush=True)\n",
    "        test_corpus = [\" \".join(lyrics) for lyrics in tqdm(test_set.loc[:, \"lyrics\"])]\n",
    "        self.test_tfidf_matrix = self.tfidf_model.tfidf_vectorizer.transform(test_corpus)\n",
    "        \n",
    "        if add_year:\n",
    "            self.train_tfidf_matrix = sparse.csr_matrix(np.hstack((self.train_tfidf_matrix.toarray(), \n",
    "                                                                train_set.loc[:, \"year\"].to_numpy().reshape((-1, 1))\n",
    "                                                                ))\n",
    "                                                       )\n",
    "            self.dev_tfidf_matrix = sparse.csr_matrix(np.hstack((self.dev_tfidf_matrix.toarray(), \n",
    "                                                             dev_set.loc[:, \"year\"].to_numpy().reshape((-1, 1))\n",
    "                                                                ))\n",
    "                                                     )\n",
    "            self.test_tfidf_matrix = sparse.csr_matrix(np.hstack((self.test_tfidf_matrix.toarray(), \n",
    "                                                                 test_set.loc[:, \"year\"].to_numpy().reshape((-1, 1))\n",
    "                                                                ))\n",
    "                                                      )\n",
    "        \n",
    "        return self.train_tfidf_matrix, self.dev_tfidf_matrix, self.test_tfidf_matrix\n",
    "\n",
    "\n",
    "    def dummy_classifier(self, train_tfidf, y_train, dev_tfidf, y_dev):\n",
    "\n",
    "        most_frequent = DummyClassifier(strategy='most_frequent')\n",
    "        most_frequent.fit(train_tfidf, y_train)\n",
    "        dumb_predictions = most_frequent.predict(dev_tfidf)\n",
    "        \n",
    "        mapping = [mapping for mapping in zip(range(len(self.genres)), self.label_encoder.inverse_transform(range(len(self.genres))))]\n",
    "        print(f\"Mapping: {mapping}\", flush=True)\n",
    "        print(classification_report(y_dev, dumb_predictions))\n",
    "        \n",
    "        return dumb_predictions\n",
    "        \n",
    "    \n",
    "    def logistic_classifier(self, train_tfidf, y_train, dev_tfidf, y_dev, \n",
    "                            regularize=False, dimensionality_reduction=False, feature_selection=False, **kwargs):\n",
    "        \n",
    "        # regularization\n",
    "        if not regularize is False:\n",
    "            best_c = None\n",
    "            best_performance = 0.0\n",
    "            for c in regularize:\n",
    "                print(c)\n",
    "                classifier_c = LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', C=c, **kwargs)\n",
    "                classifier_c.fit(train_tfidf, y_train)\n",
    "                predictions_c = classifier_c.predict(dev_tfidf)\n",
    "                score = f1_score(y_dev, predictions_c, average='micro')\n",
    "                if score > best_performance:\n",
    "                    best_performance = score\n",
    "                    best_c = c\n",
    "                    print(\"New best performance: {}\".format(score))\n",
    "\n",
    "                print(classification_report(y_dev, predictions_c))\n",
    "            return best_c\n",
    "        \n",
    "        # dimensionality reduction\n",
    "        elif not dimensionality_reduction is False:\n",
    "            best_dimension = None\n",
    "            best_performance = 0.0\n",
    "            for k in dimensionality_reduction:\n",
    "                print(k)\n",
    "                svd = TruncatedSVD(n_components=k)\n",
    "\n",
    "                X_train_dim = svd.fit_transform(train_tfidf)\n",
    "                X_dev_dim = svd.transform(dev_tfidf)\n",
    "\n",
    "                classifier_k = LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', **kwargs)\n",
    "                classifier_k.fit(train_tfidf, y_train)\n",
    "                predictions_k = classifier_k.predict(dev_tfidf)\n",
    "                score = f1_score(y_dev, predictions_k, average='micro')\n",
    "                if score > best_performance:\n",
    "                    best_performance = score\n",
    "                    best_dimension = k\n",
    "                    print(\"New best performance: {}\".format(score))\n",
    "                print(classification_report(y_dev, predictions_k))\n",
    "            \n",
    "            return best_dimension\n",
    "        \n",
    "        # feature selection\n",
    "        elif not feature_selection is False:\n",
    "            best_feature_number = None\n",
    "            best_performance = 0.0\n",
    "            for k in feature_selection:\n",
    "                print(k)\n",
    "                selector = SelectKBest(chi2, k=k).fit(train_tfidf, y_train)\n",
    "                X_train_sel = selector.transform(train_tfidf)\n",
    "                X_dev_sel = selector.transform(dev_tfidf)\n",
    "                \n",
    "                classifier_k = LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', **kwargs)\n",
    "                classifier_k.fit(X_train_sel, y_train)\n",
    "                predictions_k = classifier_k.predict(X_dev_sel)\n",
    "                score = f1_score(y_dev, predictions_k, average='micro')\n",
    "                if score > best_performance:\n",
    "                    best_performance = score\n",
    "                    best_feature_number = k\n",
    "                    print(\"New best performance: {}\".format(score))\n",
    "                print(classification_report(y_dev, predictions_k))\n",
    "                \n",
    "            return best_feature_number\n",
    "        \n",
    "        # simple logistic regression\n",
    "        else:\n",
    "            classifier = LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', **kwargs)\n",
    "            classifier.fit(train_tfidf, y_train)\n",
    "\n",
    "            predictions = classifier.predict(dev_tfidf)\n",
    "            mapping = [mapping for mapping in zip(range(len(self.genres)), self.label_encoder.inverse_transform(range(len(self.genres))))]\n",
    "            print(f\"Mapping: {mapping}\", flush=True)\n",
    "            print(classification_report(y_dev, predictions))\n",
    "            \n",
    "            return predictions\n",
    "            \n",
    "            \n",
    "    def bootstrap_sample(self, system1, system2, gold, samples=1000, score=f1_score, average='micro'):\n",
    "        \"\"\"\n",
    "        compute the proportion of times the performance difference of the \n",
    "        two systems on a subsample is significantly different from the \n",
    "        performance on the entire sample\n",
    "        \"\"\"\n",
    "        N = len(gold) # number of instances\n",
    "\n",
    "        # make sure the two systems have the same number of samples\n",
    "        assert len(system1) == N and len(system2) == N, 'samples have different lengths'\n",
    "\n",
    "        # compute performance score on entire sample\n",
    "        base_score1 = score(gold, system1, average=average)\n",
    "        base_score2 = score(gold, system2, average=average)\n",
    "\n",
    "        # switch systems if system2 is better\n",
    "        if base_score2 > base_score1:\n",
    "            system1, system2 = system2, system1\n",
    "            base_score1, base_score2 = base_score2, base_score1\n",
    "\n",
    "        # compute the difference\n",
    "        basedelta = base_score1 - base_score2\n",
    "        assert basedelta > 0, 'Wrong system first, system1 needs to be better!'\n",
    "\n",
    "        system1 = np.array(system1)\n",
    "        system2 = np.array(system2)\n",
    "        gold = np.array(gold)\n",
    "\n",
    "        p = 0\n",
    "        deltas = []\n",
    "        for i in range(samples):\n",
    "            # select a subsample, with replacement\n",
    "            sample = np.random.choice(N, size=N, replace=True)\n",
    "\n",
    "            # collect data corresponding to subsample\n",
    "            sample1 = system1[sample]\n",
    "            sample2 = system2[sample]\n",
    "            gold_sample = gold[sample]\n",
    "\n",
    "            # compute scores on subsample\n",
    "            sample_score1 = score(gold_sample, sample1, average=average)\n",
    "            sample_score2 = score(gold_sample, sample2, average=average)\n",
    "            sample_delta = sample_score1 - sample_score2\n",
    "\n",
    "            # check whether the observed sample difference is at least \n",
    "            # twice as large as the base difference\n",
    "            if sample_delta > 2*basedelta:\n",
    "                p += 1\n",
    "            deltas.append(sample_delta)\n",
    "\n",
    "        return p/samples, deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df = lyrics_df = pd.read_csv(\"./data/lyrics_cleaned.csv\")\n",
    "\n",
    "with open('./data/lemmatized_corpus.pickle', 'rb') as pickled_object:\n",
    "    lemmatized_corpus = pickle.load(pickled_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put preprocessed corpus into dataframe\n",
    "lyrics_df.loc[:, \"lyrics\"] = lemmatized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>album</th>\n",
       "      <th>release_date</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>Bruce Springsteen</td>\n",
       "      <td>Blood Brothers</td>\n",
       "      <td>Greatest Hits (1995)</td>\n",
       "      <td>1996-03-03</td>\n",
       "      <td>Rock</td>\n",
       "      <td>[-PRON-, play, king, of, the, mountain, out, o...</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>Bruce Springsteen</td>\n",
       "      <td>Lonesome Day</td>\n",
       "      <td>The Rising</td>\n",
       "      <td>2002-07-30</td>\n",
       "      <td>Rock</td>\n",
       "      <td>[baby, once, think, know, everything, need, to...</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>Bruce Springsteen</td>\n",
       "      <td>My Fathers House</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>1982-09-30</td>\n",
       "      <td>Rock</td>\n",
       "      <td>[last, night, dream, that, be, child, out, whe...</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>Bruce Springsteen</td>\n",
       "      <td>Living Proof</td>\n",
       "      <td>Lucky Town</td>\n",
       "      <td>1992-03-31</td>\n",
       "      <td>Rock</td>\n",
       "      <td>[well, now, on, summer, night, in, dusky, room...</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>Bruce Springsteen</td>\n",
       "      <td>She's The One</td>\n",
       "      <td>Born to Run</td>\n",
       "      <td>1975-08-25</td>\n",
       "      <td>Rock</td>\n",
       "      <td>[with, -PRON-, killer, grace, and, -PRON-, sec...</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>Bruce Springsteen</td>\n",
       "      <td>Native American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rock</td>\n",
       "      <td>[jimmy, fallon, emmy, award, and, grammy, awar...</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4754</th>\n",
       "      <td>Bruce Springsteen</td>\n",
       "      <td>The Fever</td>\n",
       "      <td>18 Tracks</td>\n",
       "      <td>1999-04-13</td>\n",
       "      <td>Rock</td>\n",
       "      <td>[when, get, home, from, -PRON-, job, turn, on,...</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4755</th>\n",
       "      <td>Bruce Springsteen</td>\n",
       "      <td>Oh, Mary, Don't You Weep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rock</td>\n",
       "      <td>[well, if, could, surely, would, stand, on, th...</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4756</th>\n",
       "      <td>Bruce Springsteen</td>\n",
       "      <td>Cover Me</td>\n",
       "      <td>Born in the U.S.A.</td>\n",
       "      <td>1984-07-31</td>\n",
       "      <td>Rock</td>\n",
       "      <td>[the, time, be, tough, now, just, get, tough, ...</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4757</th>\n",
       "      <td>Bruce Springsteen</td>\n",
       "      <td>Out of Work</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rock</td>\n",
       "      <td>[when, somebody, like, harry, belafonte, make,...</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4758</th>\n",
       "      <td>Bruce Springsteen</td>\n",
       "      <td>Racing In The Street</td>\n",
       "      <td>Darkness on the Edge of Town</td>\n",
       "      <td>1978-06-02</td>\n",
       "      <td>Rock</td>\n",
       "      <td>[get, sixty, nine, chevy, with, fuelie, head, ...</td>\n",
       "      <td>1978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 artist                      song  \\\n",
       "4748  Bruce Springsteen            Blood Brothers   \n",
       "4749  Bruce Springsteen              Lonesome Day   \n",
       "4750  Bruce Springsteen          My Fathers House   \n",
       "4751  Bruce Springsteen              Living Proof   \n",
       "4752  Bruce Springsteen             She's The One   \n",
       "4753  Bruce Springsteen           Native American   \n",
       "4754  Bruce Springsteen                 The Fever   \n",
       "4755  Bruce Springsteen  Oh, Mary, Don't You Weep   \n",
       "4756  Bruce Springsteen                  Cover Me   \n",
       "4757  Bruce Springsteen               Out of Work   \n",
       "4758  Bruce Springsteen      Racing In The Street   \n",
       "\n",
       "                             album release_date genre  \\\n",
       "4748          Greatest Hits (1995)   1996-03-03  Rock   \n",
       "4749                    The Rising   2002-07-30  Rock   \n",
       "4750                      Nebraska   1982-09-30  Rock   \n",
       "4751                    Lucky Town   1992-03-31  Rock   \n",
       "4752                   Born to Run   1975-08-25  Rock   \n",
       "4753                           NaN          NaN  Rock   \n",
       "4754                     18 Tracks   1999-04-13  Rock   \n",
       "4755                           NaN          NaN  Rock   \n",
       "4756            Born in the U.S.A.   1984-07-31  Rock   \n",
       "4757                           NaN          NaN  Rock   \n",
       "4758  Darkness on the Edge of Town   1978-06-02  Rock   \n",
       "\n",
       "                                                 lyrics  year  \n",
       "4748  [-PRON-, play, king, of, the, mountain, out, o...  1996  \n",
       "4749  [baby, once, think, know, everything, need, to...  2002  \n",
       "4750  [last, night, dream, that, be, child, out, whe...  1982  \n",
       "4751  [well, now, on, summer, night, in, dusky, room...  1992  \n",
       "4752  [with, -PRON-, killer, grace, and, -PRON-, sec...  1975  \n",
       "4753  [jimmy, fallon, emmy, award, and, grammy, awar...  2002  \n",
       "4754  [when, get, home, from, -PRON-, job, turn, on,...  1999  \n",
       "4755  [well, if, could, surely, would, stand, on, th...  1986  \n",
       "4756  [the, time, be, tough, now, just, get, tough, ...  1984  \n",
       "4757  [when, somebody, like, harry, belafonte, make,...  1989  \n",
       "4758  [get, sixty, nine, chevy, with, fuelie, head, ...  1978  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_df.loc[4748:4758, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_classification = LyricsClassification(lyrics_df, genres=['Hip-Hop','Rock','Pop','Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Hip-Hop', 'Rock', 'Pop', 'Country'], dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_classification.lyrics_df.genre.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24093, 7), (5163, 7), (5163, 7))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, dev_set, test_set = lyrics_classification.divide_dataset(lyrics_classification.lyrics_df)\n",
    "train_set.shape, dev_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24093,), (5163,), (5163,))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, y_dev, y_test = lyrics_classification.encode_labels(train_set, dev_set, test_set, target_variable='genre')\n",
    "y_train.shape, y_dev.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming train ...\n",
      "Joining tokens for each lyrics ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/24093 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 1911/24093 [00:00<00:01, 19107.10it/s]\u001b[A\n",
      " 16%|█▋        | 3929/24093 [00:00<00:01, 19414.07it/s]\u001b[A\n",
      " 25%|██▍       | 6020/24093 [00:00<00:00, 19838.56it/s]\u001b[A\n",
      " 34%|███▎      | 8127/24093 [00:00<00:00, 20190.71it/s]\u001b[A\n",
      " 43%|████▎     | 10366/24093 [00:00<00:00, 20801.03it/s]\u001b[A\n",
      " 52%|█████▏    | 12493/24093 [00:00<00:00, 20934.53it/s]\u001b[A\n",
      " 60%|██████    | 14511/24093 [00:00<00:00, 20699.43it/s]\u001b[A\n",
      " 69%|██████▉   | 16655/24093 [00:00<00:00, 20912.87it/s]\u001b[A\n",
      " 79%|███████▉  | 19058/24093 [00:00<00:00, 21757.58it/s]\u001b[A\n",
      " 89%|████████▊ | 21367/24093 [00:01<00:00, 22138.51it/s]\u001b[A\n",
      " 98%|█████████▊| 23537/24093 [00:01<00:00, 21985.52it/s]\u001b[A\n",
      "100%|██████████| 24093/24093 [00:01<00:00, 21259.23it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TFIDF vectorizer ...\n",
      "Transforming dev ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5163 [00:00<?, ?it/s]\u001b[A\n",
      " 37%|███▋      | 1899/5163 [00:00<00:00, 18988.89it/s]\u001b[A\n",
      " 76%|███████▌  | 3918/5163 [00:00<00:00, 19332.16it/s]\u001b[A\n",
      "100%|██████████| 5163/5163 [00:00<00:00, 19718.68it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5163 [00:00<?, ?it/s]\u001b[A\n",
      " 46%|████▌     | 2373/5163 [00:00<00:00, 23729.23it/s]\u001b[A\n",
      " 93%|█████████▎| 4788/5163 [00:00<00:00, 23851.58it/s]\u001b[A\n",
      "100%|██████████| 5163/5163 [00:00<00:00, 23790.33it/s]\u001b[A"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((24093, 166887), (5163, 166887), (5163, 166887))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf, dev_tfidf, test_tfidf = lyrics_classification.transform_input(train_set, dev_set, test_set, \n",
    "                                                                           analyzer='char', ngram_range=(2,6), add_year=False)\n",
    "train_tfidf.shape, dev_tfidf.shape, test_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping: [(0, 'Country'), (1, 'Hip-Hop'), (2, 'Pop'), (3, 'Rock')]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       550\n",
      "           1       0.00      0.00      0.00       857\n",
      "           2       0.00      0.00      0.00      1469\n",
      "           3       0.44      1.00      0.61      2287\n",
      "\n",
      "    accuracy                           0.44      5163\n",
      "   macro avg       0.11      0.25      0.15      5163\n",
      "weighted avg       0.20      0.44      0.27      5163\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucamasserano/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "dumb_predictions = lyrics_classification.dummy_classifier(train_tfidf, y_train, dev_tfidf, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping: [(0, 'Country'), (1, 'Hip-Hop'), (2, 'Pop'), (3, 'Rock')]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.24      0.36       550\n",
      "           1       0.82      0.70      0.76       857\n",
      "           2       0.59      0.46      0.52      1469\n",
      "           3       0.62      0.84      0.72      2287\n",
      "\n",
      "    accuracy                           0.65      5163\n",
      "   macro avg       0.69      0.56      0.59      5163\n",
      "weighted avg       0.66      0.65      0.63      5163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_predictions = lyrics_classification.logistic_classifier(train_tfidf=train_tfidf, y_train=y_train, \n",
    "                                                                 dev_tfidf=dev_tfidf, y_dev=y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping: [(0, 'Country'), (1, 'Hip-Hop'), (2, 'Pop'), (3, 'Rock')]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.64      0.54       550\n",
      "           1       0.74      0.80      0.77       857\n",
      "           2       0.56      0.54      0.55      1469\n",
      "           3       0.70      0.64      0.67      2287\n",
      "\n",
      "    accuracy                           0.64      5163\n",
      "   macro avg       0.62      0.65      0.63      5163\n",
      "weighted avg       0.64      0.64      0.64      5163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_predictions_balanced = lyrics_classification.logistic_classifier(train_tfidf=train_tfidf, y_train=y_train, \n",
    "                                                                  dev_tfidf=dev_tfidf, y_dev=y_dev, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "New best performance: 0.6159209761766414\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.38      0.45       550\n",
      "           1       0.78      0.69      0.73       857\n",
      "           2       0.51      0.51      0.51      1469\n",
      "           3       0.64      0.71      0.67      2287\n",
      "\n",
      "    accuracy                           0.62      5163\n",
      "   macro avg       0.62      0.57      0.59      5163\n",
      "weighted avg       0.62      0.62      0.61      5163\n",
      "\n",
      "20\n",
      "New best performance: 0.6261863257795856\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.43      0.49       550\n",
      "           1       0.81      0.70      0.75       857\n",
      "           2       0.53      0.50      0.51      1469\n",
      "           3       0.64      0.72      0.68      2287\n",
      "\n",
      "    accuracy                           0.63      5163\n",
      "   macro avg       0.63      0.59      0.61      5163\n",
      "weighted avg       0.63      0.63      0.62      5163\n",
      "\n",
      "10\n",
      "New best performance: 0.631222157660275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.38      0.46       550\n",
      "           1       0.80      0.70      0.75       857\n",
      "           2       0.54      0.50      0.52      1469\n",
      "           3       0.64      0.75      0.69      2287\n",
      "\n",
      "    accuracy                           0.63      5163\n",
      "   macro avg       0.64      0.58      0.60      5163\n",
      "weighted avg       0.63      0.63      0.63      5163\n",
      "\n",
      "5\n",
      "New best performance: 0.6343211311253147\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.35      0.45       550\n",
      "           1       0.81      0.70      0.75       857\n",
      "           2       0.55      0.48      0.51      1469\n",
      "           3       0.63      0.78      0.70      2287\n",
      "\n",
      "    accuracy                           0.63      5163\n",
      "   macro avg       0.65      0.58      0.60      5163\n",
      "weighted avg       0.63      0.63      0.63      5163\n",
      "\n",
      "2\n",
      "New best performance: 0.6459422816192136\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.30      0.41       550\n",
      "           1       0.81      0.71      0.76       857\n",
      "           2       0.58      0.48      0.53      1469\n",
      "           3       0.63      0.81      0.71      2287\n",
      "\n",
      "    accuracy                           0.65      5163\n",
      "   macro avg       0.66      0.58      0.60      5163\n",
      "weighted avg       0.65      0.65      0.63      5163\n",
      "\n",
      "0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.13      0.22       550\n",
      "           1       0.82      0.70      0.75       857\n",
      "           2       0.60      0.43      0.50      1469\n",
      "           3       0.61      0.87      0.71      2287\n",
      "\n",
      "    accuracy                           0.64      5163\n",
      "   macro avg       0.68      0.53      0.55      5163\n",
      "weighted avg       0.65      0.64      0.61      5163\n",
      "\n",
      "0.1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00       550\n",
      "           1       0.81      0.67      0.73       857\n",
      "           2       0.60      0.32      0.42      1469\n",
      "           3       0.57      0.91      0.70      2287\n",
      "\n",
      "    accuracy                           0.60      5163\n",
      "   macro avg       0.74      0.47      0.46      5163\n",
      "weighted avg       0.66      0.60      0.55      5163\n",
      "\n",
      "0.05\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       550\n",
      "           1       0.82      0.62      0.71       857\n",
      "           2       0.61      0.24      0.35      1469\n",
      "           3       0.55      0.94      0.69      2287\n",
      "\n",
      "    accuracy                           0.59      5163\n",
      "   macro avg       0.49      0.45      0.44      5163\n",
      "weighted avg       0.55      0.59      0.52      5163\n",
      "\n",
      "0.01\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       550\n",
      "           1       0.86      0.21      0.34       857\n",
      "           2       0.81      0.01      0.02      1469\n",
      "           3       0.46      1.00      0.63      2287\n",
      "\n",
      "    accuracy                           0.48      5163\n",
      "   macro avg       0.53      0.30      0.25      5163\n",
      "weighted avg       0.58      0.48      0.34      5163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_c = lyrics_classification.logistic_classifier(train_tfidf=train_tfidf, y_train=y_train, dev_tfidf=dev_tfidf, y_dev=y_dev, \n",
    "                                                  regularize=[50, 20, 10, 5, 2, 0.5, 0.1, 0.05, 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.30      0.41       550\n",
      "           1       0.81      0.71      0.76       857\n",
      "           2       0.58      0.48      0.53      1469\n",
      "           3       0.63      0.81      0.71      2287\n",
      "\n",
      "    accuracy                           0.65      5163\n",
      "   macro avg       0.66      0.58      0.60      5163\n",
      "weighted avg       0.65      0.65      0.63      5163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_c_classifier = LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', C=best_c)\n",
    "\n",
    "best_c_classifier.fit(train_tfidf, y_train)\n",
    "predictions_c_dev = best_c_classifier.predict(dev_tfidf)\n",
    "\n",
    "print(classification_report(y_dev, predictions_c_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.37      0.49       550\n",
      "           1       0.78      0.74      0.76       857\n",
      "           2       0.58      0.51      0.54      1470\n",
      "           3       0.65      0.79      0.71      2286\n",
      "\n",
      "    accuracy                           0.66      5163\n",
      "   macro avg       0.68      0.60      0.63      5163\n",
      "weighted avg       0.66      0.66      0.65      5163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_c_classifier = LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', C=best_c)\n",
    "\n",
    "best_c_classifier.fit(train_tfidf, y_train)\n",
    "predictions_c_test = best_c_classifier.predict(test_tfidf)\n",
    "\n",
    "print(classification_report(y_test, predictions_c_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best classifier is with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "New best performance: 0.6401317063722642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.23      0.34       550\n",
      "           1       0.82      0.71      0.76       857\n",
      "           2       0.59      0.45      0.51      1469\n",
      "           3       0.61      0.84      0.71      2287\n",
      "\n",
      "    accuracy                           0.64      5163\n",
      "   macro avg       0.67      0.56      0.58      5163\n",
      "weighted avg       0.65      0.64      0.62      5163\n",
      "\n",
      "50000\n",
      "New best performance: 0.6472980825101685\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.27      0.39       550\n",
      "           1       0.82      0.72      0.76       857\n",
      "           2       0.58      0.46      0.51      1469\n",
      "           3       0.63      0.83      0.72      2287\n",
      "\n",
      "    accuracy                           0.65      5163\n",
      "   macro avg       0.68      0.57      0.60      5163\n",
      "weighted avg       0.65      0.65      0.63      5163\n",
      "\n",
      "70000\n",
      "New best performance: 0.6488475692426884\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.29      0.41       550\n",
      "           1       0.82      0.71      0.76       857\n",
      "           2       0.59      0.47      0.52      1469\n",
      "           3       0.63      0.82      0.71      2287\n",
      "\n",
      "    accuracy                           0.65      5163\n",
      "   macro avg       0.68      0.58      0.60      5163\n",
      "weighted avg       0.65      0.65      0.63      5163\n",
      "\n",
      "90000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.31      0.42       550\n",
      "           1       0.81      0.71      0.76       857\n",
      "           2       0.58      0.47      0.52      1469\n",
      "           3       0.63      0.82      0.71      2287\n",
      "\n",
      "    accuracy                           0.65      5163\n",
      "   macro avg       0.67      0.58      0.60      5163\n",
      "weighted avg       0.65      0.65      0.64      5163\n",
      "\n",
      "110000\n",
      "New best performance: 0.6492349409258183\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.30      0.41       550\n",
      "           1       0.81      0.71      0.76       857\n",
      "           2       0.58      0.48      0.53      1469\n",
      "           3       0.63      0.82      0.72      2287\n",
      "\n",
      "    accuracy                           0.65      5163\n",
      "   macro avg       0.67      0.58      0.60      5163\n",
      "weighted avg       0.65      0.65      0.64      5163\n",
      "\n",
      "130000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.30      0.41       550\n",
      "           1       0.81      0.72      0.76       857\n",
      "           2       0.58      0.48      0.53      1469\n",
      "           3       0.63      0.82      0.71      2287\n",
      "\n",
      "    accuracy                           0.65      5163\n",
      "   macro avg       0.67      0.58      0.60      5163\n",
      "weighted avg       0.65      0.65      0.64      5163\n",
      "\n",
      "166887\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.30      0.41       550\n",
      "           1       0.81      0.71      0.76       857\n",
      "           2       0.58      0.48      0.53      1469\n",
      "           3       0.63      0.81      0.71      2287\n",
      "\n",
      "    accuracy                           0.65      5163\n",
      "   macro avg       0.66      0.58      0.60      5163\n",
      "weighted avg       0.65      0.65      0.63      5163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_k = lyrics_classification.logistic_classifier(train_tfidf=train_tfidf, y_train=y_train, dev_tfidf=dev_tfidf, y_dev=y_dev, \n",
    "                                                  feature_selection=[30000, 50000, 70000, 90000, 110000, 130000, 166887], C=best_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110000"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2, k=best_k).fit(train_tfidf, y_train)\n",
    "X_train_sel = selector.transform(train_tfidf)\n",
    "X_dev_sel = selector.transform(dev_tfidf)\n",
    "X_test_sel = selector.transform(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.36      0.48       550\n",
      "           1       0.79      0.74      0.76       857\n",
      "           2       0.59      0.50      0.54      1470\n",
      "           3       0.65      0.80      0.72      2286\n",
      "\n",
      "    accuracy                           0.66      5163\n",
      "   macro avg       0.68      0.60      0.62      5163\n",
      "weighted avg       0.66      0.66      0.65      5163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_k_classifier = LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', C=best_c)\n",
    "\n",
    "best_k_classifier.fit(X_train_sel, y_train)\n",
    "predictions_k_test = best_k_classifier.predict(X_test_sel)\n",
    "\n",
    "print(classification_report(y_test, predictions_k_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "best_dimension = lyrics_classification.logistic_classifier(train_tfidf=X_train_sel, y_train=y_train, \n",
    "                                                           dev_tfidf=X_dev_sel, y_dev=y_dev, \n",
    "                                                    dimensionality_reduction=[10000, 20000, 30000, 40000, 50000, 80000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ciao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=k)\n",
    "\n",
    "                X_train_dim = svd.fit_transform(train_tfidf)\n",
    "                X_dev_dim = svd.transform(dev_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest.fit(train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions = random_forest.predict(dev_tfidf)\n",
    "rf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_dev, rf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value, deltas = lyrics_classification.bootstrap_sample(system1=predictions_logit_default.tolist(), system2=dumb_predictions.tolist(), gold=y_dev.tolist())\n",
    "print(p_value, p_value < 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_dev, dumb_predictions, average='micro'), f1_score(y_dev, predictions_c_dev, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value, deltas = lyrics_classification.bootstrap_sample(system1=predictions_c_dev.tolist(), system2=dumb_predictions.tolist(), gold=y_dev.tolist())\n",
    "print(p_value, p_value < 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_dev, dumb_predictions, average='micro'), f1_score(y_dev, predictions_c_dev, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.Series(deltas).plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
