{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None) # visualize all columns in console\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LyricsTFIDF CLASS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricsTFIDF:\n",
    "    \n",
    "    def __init__(self, lyrics_df, preprocessed_corpus):\n",
    "        \n",
    "        self.lyrics_df = lyrics_df\n",
    "        self.preprocessed_corpus = preprocessed_corpus\n",
    "        self.tfidf_matrix = None\n",
    "        \n",
    "    def fit_transform(self, \n",
    "                      preprocessed_corpus=None, \n",
    "                      compute_constituents=False,\n",
    "                      save_attributes=True,\n",
    "                      topn=20,\n",
    "                      analyzer='word',\n",
    "                      ngram_range=(1,1), \n",
    "                      doc_frequency=(0.001,0.75), \n",
    "                      stop_words_language='english', \n",
    "                      logarithmic=True, \n",
    "                      info=True):\n",
    "        \n",
    "        if preprocessed_corpus is None:\n",
    "            preprocessed_corpus = self.preprocessed_corpus\n",
    "        \n",
    "        # preprocessed corpus is expected to be a list of list of tokens/lemmas\n",
    "        if info:\n",
    "            print(\"Joining tokens for each lyrics ...\", flush=True)\n",
    "        preprocessed_corpus = [\" \".join(lyrics) for lyrics in tqdm(preprocessed_corpus)]\n",
    "        \n",
    "        # dict for dataframe\n",
    "        data = {}\n",
    "        \n",
    "        # initialize\n",
    "        if info:\n",
    "            print(\"Fitting TFIDF vectorizer ...\", flush=True)\n",
    "        tfidf_vectorizer = TfidfVectorizer(analyzer=analyzer,\n",
    "                                            ngram_range=ngram_range,\n",
    "                                            min_df=doc_frequency[0], \n",
    "                                            max_df=doc_frequency[1], \n",
    "                                            stop_words=stop_words_language, \n",
    "                                            sublinear_tf=logarithmic\n",
    "                                           )\n",
    "        # fit-transform\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_corpus)\n",
    "    \n",
    "        if save_attributes:\n",
    "            self.tfidf_matrix = tfidf_matrix\n",
    "            self.tfidf_vectorizer = tfidf_vectorizer\n",
    "            \n",
    "        data['ngram'] = tfidf_vectorizer.get_feature_names()\n",
    "        data['tfidf'] = tfidf_matrix.sum(axis=0).A1\n",
    "        \n",
    "        if compute_constituents:\n",
    "            if info:\n",
    "                print(\"Fitting count vectorizer ...\", flush=True)\n",
    "            count_vectorizer = CountVectorizer(analyzer=analyzer,\n",
    "                                                ngram_range=ngram_range,\n",
    "                                                min_df=doc_frequency[0], \n",
    "                                                max_df=doc_frequency[1], \n",
    "                                                stop_words=stop_words_language, \n",
    "                                                sublinear_tf=logarithmic\n",
    "                                               )\n",
    "            count_matrix = count_vectorizer.fit_transform(preprocessed_corpus)\n",
    "            \n",
    "            data['tf'] = count_matrix.sum(axis=0).A1\n",
    "            data['idf'] = tfidf_vectorizer_word.idf_\n",
    "                \n",
    "        # create DataFrame\n",
    "        df = pd.DataFrame(data=data).sort_values(\"tfidf\", ascending=False).reset_index(drop=True).loc[:topn, :]\n",
    "        \n",
    "        return df\n",
    "        \n",
    "        \n",
    "    def tfidf_by_genre(self, genres, lyrics_df=None, preprocessed_corpus=None, **kwargs):\n",
    "        \n",
    "        if lyrics_df is None:\n",
    "            lyrics_df = self.lyrics_df\n",
    "        if preprocessed_corpus is None:\n",
    "            preprocessed_corpus = self.preprocessed_corpus\n",
    "        \n",
    "        # check if input error\n",
    "        for genre in genres:\n",
    "            admissible = lyrics_df.genre.unique()\n",
    "            if genre not in admissible:\n",
    "                raise KeyError(f\"{genre} is not an admissible genre\")\n",
    "        \n",
    "        # initialize df to return\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        print(\"Fitting TFIDF vectorizer for all genres ...\", flush=True)\n",
    "        for genre in genres:\n",
    "\n",
    "            genre_idxs = lyrics_df.loc[lyrics_df.genre == genre, \"lyrics\"].index\n",
    "            genre_corpus = np.array(preprocessed_corpus)[genre_idxs].tolist()\n",
    "            \n",
    "            genre_df = self.fit_transform(preprocessed_corpus=genre_corpus, info=False, save_attributes=False, **kwargs)\n",
    "            \n",
    "            # add MultiIndex for genres\n",
    "            genre_df.columns = pd.MultiIndex.from_product([[genre], genre_df.columns])\n",
    "            \n",
    "            df = pd.concat([df, genre_df], axis=\"columns\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def matrix_factorization(self, method, n_components, tfidf_matrix=None):\n",
    "        \n",
    "        if tfidf_matrix is None:\n",
    "            tfidf_matrix = self.tfidf_matrix\n",
    "        \n",
    "        if method == 'svd':\n",
    "            svd = TruncatedSVD(n_components=n_components)\n",
    "            U = svd.fit_transform(tfidf_matrix)\n",
    "            S = svd.singular_values_\n",
    "            V = svd.components_\n",
    "            \n",
    "            return U, S, V\n",
    "\n",
    "        elif method == 'nmf':\n",
    "            nmf = NMF(n_components=n_components, init='nndsvd', random_state=0)\n",
    "\n",
    "            W = nmf.fit_transform(tfidf_matrix)\n",
    "            H = nmf.components_\n",
    "            \n",
    "            return W, H\n",
    "    \n",
    "    \n",
    "    def print_latent_topics(self, lower_dimensional_words, vocabulary=None, topn=5):\n",
    "    \n",
    "        if vocabulary is None:\n",
    "            vocabulary = self.tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "        topic_words = ([[vocabulary[i] for i in np.argsort(t)[:-topn-1:-1]]\n",
    "                        for t in lower_dimensional_words])\n",
    "        \n",
    "        return [', '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43844, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_df = pd.read_csv(\"/Users/lucamasserano/Desktop/BOCCONI/nlp/final_project/lyrics_project/data/lyrics_cleaned.csv\")\n",
    "lyrics_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./data/trimmed_corpus_old.pickle', 'rb') as pickled_object:\n",
    "#    trimmed_corpus_old = pickle.load(pickled_object)\n",
    "with open('./data/trimmed_corpus.pickle', 'rb') as pickled_object:\n",
    "    trimmed_corpus = pickle.load(pickled_object)\n",
    "\n",
    "# remove if len(token) < 2 --> PUT THIS IN PREPROCESSING AND GIVE STATISTICS ON NUMBER OF LEMMA REMOVED\n",
    "#trimmed_corpus = [[lemma for lemma in lyrics if len(lemma) > 2] for lyrics in trimmed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['appreciate',\n",
       " 'when',\n",
       " 'young',\n",
       " 'mama',\n",
       " 'beef',\n",
       " 'year',\n",
       " 'old',\n",
       " 'kick',\n",
       " 'street',\n",
       " 'back',\n",
       " 'time',\n",
       " 'never',\n",
       " 'think',\n",
       " 'see',\n",
       " 'face',\n",
       " 'woman',\n",
       " 'alive',\n",
       " 'could',\n",
       " 'take',\n",
       " 'mama',\n",
       " 'place',\n",
       " 'suspend',\n",
       " 'school',\n",
       " 'scare',\n",
       " 'home',\n",
       " 'fool',\n",
       " 'big',\n",
       " 'boy',\n",
       " 'breakin',\n",
       " 'rule',\n",
       " 'shed',\n",
       " 'tear',\n",
       " 'baby',\n",
       " 'sister',\n",
       " 'year',\n",
       " 'poor',\n",
       " 'other',\n",
       " 'little',\n",
       " 'kid',\n",
       " 'even',\n",
       " 'different',\n",
       " 'daddy',\n",
       " 'same',\n",
       " 'drama',\n",
       " 'when',\n",
       " 'thing',\n",
       " 'go',\n",
       " 'wrong',\n",
       " 'blame',\n",
       " 'reminisce',\n",
       " 'stress',\n",
       " 'cause',\n",
       " 'hell',\n",
       " 'huggin',\n",
       " 'mama',\n",
       " 'jail',\n",
       " 'cell',\n",
       " 'think',\n",
       " 'elementary',\n",
       " 'hey',\n",
       " 'see',\n",
       " 'penitentiary',\n",
       " 'day',\n",
       " 'police',\n",
       " 'right',\n",
       " 'mama',\n",
       " 'catch',\n",
       " 'put',\n",
       " 'whoopin',\n",
       " 'backside',\n",
       " 'even',\n",
       " 'crack',\n",
       " 'fiend',\n",
       " 'mama',\n",
       " 'always',\n",
       " 'black',\n",
       " 'queen',\n",
       " 'finally',\n",
       " 'understand',\n",
       " 'woman',\n",
       " 'easy',\n",
       " 'tryin',\n",
       " 'raise',\n",
       " 'man',\n",
       " 'always',\n",
       " 'commit',\n",
       " 'poor',\n",
       " 'single',\n",
       " 'mother',\n",
       " 'welfare',\n",
       " 'tell',\n",
       " 'how',\n",
       " 'way',\n",
       " 'can',\n",
       " 'pay',\n",
       " 'back',\n",
       " 'plan',\n",
       " 'show',\n",
       " 'understand',\n",
       " 'appreciate',\n",
       " 'lady',\n",
       " 'know',\n",
       " 'love',\n",
       " 'dear',\n",
       " 'sweet',\n",
       " 'lady',\n",
       " 'place',\n",
       " 'one',\n",
       " 'appreciate',\n",
       " 'sweet',\n",
       " 'lady',\n",
       " 'know',\n",
       " 'love',\n",
       " 'now',\n",
       " 'tell',\n",
       " 'fair',\n",
       " 'love',\n",
       " 'daddy',\n",
       " 'cause',\n",
       " 'coward',\n",
       " 'there',\n",
       " 'pass',\n",
       " 'cry',\n",
       " 'cause',\n",
       " 'anger',\n",
       " 'would',\n",
       " 'let',\n",
       " 'feel',\n",
       " 'stranger',\n",
       " 'say',\n",
       " 'wrong',\n",
       " 'heartless',\n",
       " 'father',\n",
       " 'go',\n",
       " 'hang',\n",
       " 'thug',\n",
       " 'even',\n",
       " 'sell',\n",
       " 'drug',\n",
       " 'show',\n",
       " 'young',\n",
       " 'brother',\n",
       " 'love',\n",
       " 'move',\n",
       " 'start',\n",
       " 'really',\n",
       " 'need',\n",
       " 'money',\n",
       " 'own',\n",
       " 'start',\n",
       " 'slangin',\n",
       " 'guilty',\n",
       " 'cause',\n",
       " 'even',\n",
       " 'sell',\n",
       " 'rock',\n",
       " 'feel',\n",
       " 'good',\n",
       " 'puttin',\n",
       " 'money',\n",
       " 'mailbox',\n",
       " 'love',\n",
       " 'payin',\n",
       " 'rent',\n",
       " 'when',\n",
       " 'rent',\n",
       " 'due',\n",
       " 'hope',\n",
       " 'get',\n",
       " 'diamond',\n",
       " 'necklace',\n",
       " 'send',\n",
       " 'cause',\n",
       " 'when',\n",
       " 'low',\n",
       " 'there',\n",
       " 'never',\n",
       " 'leave',\n",
       " 'alone',\n",
       " 'care',\n",
       " 'could',\n",
       " 'see',\n",
       " 'home',\n",
       " 'work',\n",
       " 'late',\n",
       " 'kitchen',\n",
       " 'fix',\n",
       " 'hot',\n",
       " 'plate',\n",
       " 'just',\n",
       " 'workin',\n",
       " 'scrap',\n",
       " 'give',\n",
       " 'make',\n",
       " 'miracle',\n",
       " 'thanksgivin',\n",
       " 'now',\n",
       " 'road',\n",
       " 'get',\n",
       " 'rough',\n",
       " 'alone',\n",
       " 'tryin',\n",
       " 'raise',\n",
       " 'bad',\n",
       " 'kid',\n",
       " 'own',\n",
       " 'way',\n",
       " 'can',\n",
       " 'pay',\n",
       " 'back',\n",
       " 'plan',\n",
       " 'show',\n",
       " 'understand',\n",
       " 'appreciate',\n",
       " 'lady',\n",
       " 'know',\n",
       " 'love',\n",
       " 'dear',\n",
       " 'sweet',\n",
       " 'lady',\n",
       " 'place',\n",
       " 'one',\n",
       " 'appreciate',\n",
       " 'sweet',\n",
       " 'lady',\n",
       " 'know',\n",
       " 'love',\n",
       " 'pour',\n",
       " 'liquor',\n",
       " 'reminisce',\n",
       " 'cause',\n",
       " 'drama',\n",
       " 'can',\n",
       " 'always',\n",
       " 'depend',\n",
       " 'mama',\n",
       " 'when',\n",
       " 'seem',\n",
       " 'hopeless',\n",
       " 'say',\n",
       " 'word',\n",
       " 'can',\n",
       " 'back',\n",
       " 'focus',\n",
       " 'when',\n",
       " 'sick',\n",
       " 'little',\n",
       " 'kid',\n",
       " 'keep',\n",
       " 'happy',\n",
       " 'limit',\n",
       " 'thing',\n",
       " 'childhood',\n",
       " 'memory',\n",
       " 'full',\n",
       " 'sweet',\n",
       " 'thing',\n",
       " 'even',\n",
       " 'act',\n",
       " 'crazy',\n",
       " 'get',\n",
       " 'thank',\n",
       " 'make',\n",
       " 'word',\n",
       " 'can',\n",
       " 'express',\n",
       " 'how',\n",
       " 'feel',\n",
       " 'never',\n",
       " 'keep',\n",
       " 'secret',\n",
       " 'always',\n",
       " 'stay',\n",
       " 'real',\n",
       " 'appreciate',\n",
       " 'how',\n",
       " 'raise',\n",
       " 'extra',\n",
       " 'love',\n",
       " 'give',\n",
       " 'wish',\n",
       " 'could',\n",
       " 'take',\n",
       " 'pain',\n",
       " 'away',\n",
       " 'can',\n",
       " 'make',\n",
       " 'night',\n",
       " 'bright',\n",
       " 'day',\n",
       " 'will',\n",
       " 'alright',\n",
       " 'hold',\n",
       " 'struggle',\n",
       " 'day',\n",
       " 'get',\n",
       " 'roll',\n",
       " 'way',\n",
       " 'can',\n",
       " 'pay',\n",
       " 'back',\n",
       " 'plan',\n",
       " 'show',\n",
       " 'understand',\n",
       " 'appreciate',\n",
       " 'lady',\n",
       " 'know',\n",
       " 'love',\n",
       " 'dear',\n",
       " 'sweet',\n",
       " 'lady',\n",
       " 'place',\n",
       " 'one',\n",
       " 'appreciate',\n",
       " 'sweet',\n",
       " 'lady',\n",
       " 'know',\n",
       " 'love',\n",
       " 'dear',\n",
       " 'lady',\n",
       " 'dear',\n",
       " 'lady']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed_corpus[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/tagged_corpus.pickle', 'rb') as pickled_object:\n",
    "    tagged_corpus = pickle.load(pickled_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['appreciate_VERB',\n",
       " 'when_ADV',\n",
       " 'young_ADJ',\n",
       " 'mama_NOUN',\n",
       " 'beef_NOUN',\n",
       " 'year_NOUN',\n",
       " 'old_ADJ',\n",
       " 'kick_VERB',\n",
       " 'street_NOUN',\n",
       " 'back_ADV',\n",
       " 'time_NOUN',\n",
       " 'never_ADV',\n",
       " 'think_VERB',\n",
       " 'would_VERB',\n",
       " 'see_VERB',\n",
       " 'face_NOUN',\n",
       " 'be_VERB',\n",
       " 'woman_NOUN',\n",
       " 'alive_ADJ',\n",
       " 'could_VERB',\n",
       " 'take_VERB',\n",
       " 'mama_NOUN',\n",
       " 'place_NOUN',\n",
       " 'suspend_VERB',\n",
       " 'school_NOUN',\n",
       " 'scare_VERB',\n",
       " 'go_VERB',\n",
       " 'home_ADV',\n",
       " 'fool_NOUN',\n",
       " 'big_ADJ',\n",
       " 'boy_NOUN',\n",
       " 'breakin_NOUN',\n",
       " 'rule_NOUN',\n",
       " 'shed_VERB',\n",
       " 'tear_NOUN',\n",
       " 'baby_NOUN',\n",
       " 'sister_NOUN',\n",
       " 'year_NOUN',\n",
       " 'poor_ADJ',\n",
       " 'other_ADJ',\n",
       " 'little_ADJ',\n",
       " 'kid_NOUN',\n",
       " 'even_ADV',\n",
       " 'different_ADJ',\n",
       " 'daddy_NOUN',\n",
       " 'same_ADJ',\n",
       " 'drama_NOUN',\n",
       " 'when_ADV',\n",
       " 'thing_NOUN',\n",
       " 'go_VERB',\n",
       " 'wrong_ADJ',\n",
       " 'would_VERB',\n",
       " 'blame_VERB',\n",
       " 'reminisce_VERB',\n",
       " 'stress_NOUN',\n",
       " 'cause_VERB',\n",
       " 'hell_NOUN',\n",
       " 'huggin_ADJ',\n",
       " 'mama_NOUN',\n",
       " 'jail_NOUN',\n",
       " 'cell_NOUN',\n",
       " \"'d_VERB\",\n",
       " 'think_VERB',\n",
       " 'elementary_ADJ',\n",
       " 'would_VERB',\n",
       " 'see_VERB',\n",
       " 'penitentiary_ADJ',\n",
       " 'day_NOUN',\n",
       " 'police_NOUN',\n",
       " 'right_ADJ',\n",
       " 'mama_NOUN',\n",
       " 'catch_VERB',\n",
       " 'put_VERB',\n",
       " 'whoopin_NOUN',\n",
       " 'backside_NOUN',\n",
       " 'even_ADV',\n",
       " 'crack_NOUN',\n",
       " 'fiend_NOUN',\n",
       " 'mama_VERB',\n",
       " 'always_ADV',\n",
       " 'black_ADJ',\n",
       " 'queen_NOUN',\n",
       " 'finally_ADV',\n",
       " 'understand_VERB',\n",
       " 'woman_NOUN',\n",
       " 'be_VERB',\n",
       " 'easy_ADJ',\n",
       " 'tryin_NOUN',\n",
       " 'raise_VERB',\n",
       " 'man_NOUN',\n",
       " 'always_ADV',\n",
       " 'commit_VERB',\n",
       " 'poor_ADJ',\n",
       " 'single_ADJ',\n",
       " 'mother_NOUN',\n",
       " 'welfare_NOUN',\n",
       " 'tell_VERB',\n",
       " 'how_ADV',\n",
       " 'way_NOUN',\n",
       " 'can_VERB',\n",
       " 'pay_VERB',\n",
       " 'back_ADV',\n",
       " 'plan_NOUN',\n",
       " 'show_VERB',\n",
       " 'understand_VERB',\n",
       " 'appreciate_VERB',\n",
       " 'lady_NOUN',\n",
       " 'know_VERB',\n",
       " 'love_VERB',\n",
       " 'dear_ADJ',\n",
       " 'sweet_ADJ',\n",
       " 'lady_NOUN',\n",
       " 'place_VERB',\n",
       " 'one_NOUN',\n",
       " 'appreciate_VERB',\n",
       " 'sweet_ADJ',\n",
       " 'lady_NOUN',\n",
       " 'know_VERB',\n",
       " 'love_VERB',\n",
       " 'now_ADV',\n",
       " 'be_VERB',\n",
       " 'tell_VERB',\n",
       " 'fair_ADJ',\n",
       " 'love_NOUN',\n",
       " 'daddy_NOUN',\n",
       " 'cause_VERB',\n",
       " 'coward_NOUN',\n",
       " 'there_ADV',\n",
       " 'pass_VERB',\n",
       " 'cry_VERB',\n",
       " 'cause_VERB',\n",
       " 'anger_NOUN',\n",
       " 'would_VERB',\n",
       " 'let_VERB',\n",
       " 'feel_VERB',\n",
       " 'stranger_NOUN',\n",
       " 'say_VERB',\n",
       " 'wrong_ADJ',\n",
       " 'heartless_ADJ',\n",
       " 'father_NOUN',\n",
       " 'go_VERB',\n",
       " 'hang_VERB',\n",
       " 'thug_NOUN',\n",
       " 'even_ADV',\n",
       " 'sell_VERB',\n",
       " 'drug_NOUN',\n",
       " 'show_VERB',\n",
       " 'young_ADJ',\n",
       " 'brother_NOUN',\n",
       " 'love_NOUN',\n",
       " 'move_VERB',\n",
       " 'start_VERB',\n",
       " 'really_ADV',\n",
       " 'need_VERB',\n",
       " 'money_NOUN',\n",
       " 'own_ADJ',\n",
       " 'so_ADV',\n",
       " 'start_VERB',\n",
       " 'slangin_NOUN',\n",
       " 'be_VERB',\n",
       " 'guilty_ADJ',\n",
       " 'cause_NOUN',\n",
       " 'even_ADV',\n",
       " 'sell_VERB',\n",
       " 'rock_NOUN',\n",
       " 'feel_VERB',\n",
       " 'good_ADJ',\n",
       " 'puttin_NOUN',\n",
       " 'money_NOUN',\n",
       " 'mailbox_NOUN',\n",
       " 'love_VERB',\n",
       " 'payin_NOUN',\n",
       " 'rent_NOUN',\n",
       " 'when_ADV',\n",
       " 'rent_NOUN',\n",
       " 'due_ADJ',\n",
       " 'hope_VERB',\n",
       " 'get_VERB',\n",
       " 'diamond_NOUN',\n",
       " 'necklace_NOUN',\n",
       " 'send_VERB',\n",
       " 'cause_VERB',\n",
       " 'when_ADV',\n",
       " 'low_ADJ',\n",
       " 'there_ADV',\n",
       " 'never_ADV',\n",
       " 'leave_VERB',\n",
       " 'alone_ADJ',\n",
       " 'care_VERB',\n",
       " 'could_VERB',\n",
       " 'see_VERB',\n",
       " 'home_NOUN',\n",
       " 'work_NOUN',\n",
       " 'late_ADV',\n",
       " 'kitchen_NOUN',\n",
       " 'fix_VERB',\n",
       " 'hot_ADJ',\n",
       " 'plate_NOUN',\n",
       " 'just_ADV',\n",
       " 'workin_VERB',\n",
       " 'scrap_NOUN',\n",
       " 'give_VERB',\n",
       " 'make_VERB',\n",
       " 'miracle_NOUN',\n",
       " 'thanksgivin_NOUN',\n",
       " 'now_ADV',\n",
       " 'road_NOUN',\n",
       " 'get_VERB',\n",
       " 'rough_ADJ',\n",
       " 'alone_ADJ',\n",
       " 'tryin_NOUN',\n",
       " 'raise_VERB',\n",
       " 'bad_ADJ',\n",
       " 'kid_NOUN',\n",
       " 'own_ADJ',\n",
       " 'way_NOUN',\n",
       " 'can_VERB',\n",
       " 'pay_VERB',\n",
       " 'back_ADV',\n",
       " 'plan_NOUN',\n",
       " 'show_VERB',\n",
       " 'understand_VERB',\n",
       " 'appreciate_VERB',\n",
       " 'lady_NOUN',\n",
       " 'know_VERB',\n",
       " 'love_VERB',\n",
       " 'dear_ADJ',\n",
       " 'sweet_ADJ',\n",
       " 'lady_NOUN',\n",
       " 'place_VERB',\n",
       " 'one_NOUN',\n",
       " 'appreciate_VERB',\n",
       " 'sweet_ADJ',\n",
       " 'lady_NOUN',\n",
       " 'know_VERB',\n",
       " 'love_VERB',\n",
       " 'pour_VERB',\n",
       " 'liquor_NOUN',\n",
       " 'reminisce_VERB',\n",
       " 'cause_NOUN',\n",
       " 'drama_NOUN',\n",
       " 'can_VERB',\n",
       " 'always_ADV',\n",
       " 'depend_VERB',\n",
       " 'mama_NOUN',\n",
       " 'when_ADV',\n",
       " 'seem_VERB',\n",
       " 'hopeless_ADJ',\n",
       " 'say_VERB',\n",
       " 'word_NOUN',\n",
       " 'can_VERB',\n",
       " 'back_ADV',\n",
       " 'focus_NOUN',\n",
       " 'when_ADV',\n",
       " 'sick_ADJ',\n",
       " 'little_ADJ',\n",
       " 'kid_NOUN',\n",
       " 'keep_VERB',\n",
       " 'happy_ADJ',\n",
       " 'limit_NOUN',\n",
       " 'thing_NOUN',\n",
       " 'childhood_NOUN',\n",
       " 'memory_NOUN',\n",
       " 'full_ADJ',\n",
       " 'sweet_ADJ',\n",
       " 'thing_NOUN',\n",
       " 'even_ADV',\n",
       " 'act_VERB',\n",
       " 'crazy_ADJ',\n",
       " 'get_VERB',\n",
       " 'thank_VERB',\n",
       " 'make_VERB',\n",
       " 'word_NOUN',\n",
       " 'can_VERB',\n",
       " 'express_VERB',\n",
       " 'how_ADV',\n",
       " 'feel_VERB',\n",
       " 'never_ADV',\n",
       " 'keep_VERB',\n",
       " 'secret_NOUN',\n",
       " 'always_ADV',\n",
       " 'stay_VERB',\n",
       " 'real_ADJ',\n",
       " 'appreciate_VERB',\n",
       " 'how_ADV',\n",
       " 'raise_VERB',\n",
       " 'extra_ADJ',\n",
       " 'love_NOUN',\n",
       " 'give_VERB',\n",
       " 'wish_VERB',\n",
       " 'could_VERB',\n",
       " 'take_VERB',\n",
       " 'pain_NOUN',\n",
       " 'away_ADV',\n",
       " 'can_VERB',\n",
       " 'make_VERB',\n",
       " 'night_NOUN',\n",
       " 'bright_ADJ',\n",
       " 'day_NOUN',\n",
       " 'will_VERB',\n",
       " 'alright_VERB',\n",
       " 'hold_VERB',\n",
       " 'struggle_NOUN',\n",
       " 'day_NOUN',\n",
       " 'get_VERB',\n",
       " 'roll_VERB',\n",
       " 'way_NOUN',\n",
       " 'can_VERB',\n",
       " 'pay_VERB',\n",
       " 'back_ADV',\n",
       " 'plan_NOUN',\n",
       " 'show_VERB',\n",
       " 'understand_VERB',\n",
       " 'appreciate_VERB',\n",
       " 'lady_NOUN',\n",
       " 'know_VERB',\n",
       " 'love_VERB',\n",
       " 'dear_ADJ',\n",
       " 'sweet_ADJ',\n",
       " 'lady_NOUN',\n",
       " 'place_VERB',\n",
       " 'one_NOUN',\n",
       " 'appreciate_VERB',\n",
       " 'sweet_ADJ',\n",
       " 'lady_NOUN',\n",
       " 'know_VERB',\n",
       " 'love_VERB',\n",
       " 'dear_ADJ',\n",
       " 'lady_NOUN',\n",
       " 'dear_ADJ',\n",
       " 'lady_NOUN']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_corpus[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_tfidf = LyricsTFIDF(lyrics_df=lyrics_df, preprocessed_corpus=trimmed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. TF-IDF analysis of most important words in the whole corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very informative at any ngram level. Love is predominant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining tokens for each lyrics ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43844/43844 [00:00<00:00, 184059.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TFIDF vectorizer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>1933.175291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>say</td>\n",
       "      <td>1486.987418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>come</td>\n",
       "      <td>1400.903199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>make</td>\n",
       "      <td>1359.997496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>time</td>\n",
       "      <td>1358.093903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yeah</td>\n",
       "      <td>1266.844007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>let</td>\n",
       "      <td>1263.991387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>want</td>\n",
       "      <td>1226.357716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>feel</td>\n",
       "      <td>1187.808912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>baby</td>\n",
       "      <td>1168.889552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>way</td>\n",
       "      <td>1159.495315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tell</td>\n",
       "      <td>1109.166306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>heart</td>\n",
       "      <td>1060.527618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>think</td>\n",
       "      <td>1051.637444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>day</td>\n",
       "      <td>1037.219639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>life</td>\n",
       "      <td>1035.502913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>need</td>\n",
       "      <td>1018.478580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>look</td>\n",
       "      <td>954.466857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>right</td>\n",
       "      <td>945.082025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>night</td>\n",
       "      <td>939.250337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>away</td>\n",
       "      <td>934.472742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ngram        tfidf\n",
       "0    love  1933.175291\n",
       "1     say  1486.987418\n",
       "2    come  1400.903199\n",
       "3    make  1359.997496\n",
       "4    time  1358.093903\n",
       "5    yeah  1266.844007\n",
       "6     let  1263.991387\n",
       "7    want  1226.357716\n",
       "8    feel  1187.808912\n",
       "9    baby  1168.889552\n",
       "10    way  1159.495315\n",
       "11   tell  1109.166306\n",
       "12  heart  1060.527618\n",
       "13  think  1051.637444\n",
       "14    day  1037.219639\n",
       "15   life  1035.502913\n",
       "16   need  1018.478580\n",
       "17   look   954.466857\n",
       "18  right   945.082025\n",
       "19  night   939.250337\n",
       "20   away   934.472742"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_whole_corpus = lyrics_tfidf.fit_transform(ngram_range=(1,1), doc_frequency=(0.001, 0.5), topn=20)\n",
    "tfidf_whole_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TFIDF analysis of ngrams within musical genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TFIDF vectorizer for all genres ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5712/5712 [00:00<00:00, 149234.85it/s]\n",
      "100%|██████████| 1361/1361 [00:00<00:00, 234135.09it/s]\n",
      "100%|██████████| 3666/3666 [00:00<00:00, 285137.38it/s]\n",
      "100%|██████████| 9796/9796 [00:00<00:00, 234561.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Hip-Hop</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Electronic</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Country</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Pop</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>ngram</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>ngram</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>ngram</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just</td>\n",
       "      <td>201.531755</td>\n",
       "      <td>love</td>\n",
       "      <td>62.770534</td>\n",
       "      <td>love</td>\n",
       "      <td>171.404108</td>\n",
       "      <td>love</td>\n",
       "      <td>504.436677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>183.391465</td>\n",
       "      <td>know</td>\n",
       "      <td>57.876466</td>\n",
       "      <td>know</td>\n",
       "      <td>145.763925</td>\n",
       "      <td>know</td>\n",
       "      <td>472.331997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>say</td>\n",
       "      <td>180.236696</td>\n",
       "      <td>just</td>\n",
       "      <td>46.493119</td>\n",
       "      <td>just</td>\n",
       "      <td>143.931513</td>\n",
       "      <td>just</td>\n",
       "      <td>392.819332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bitch</td>\n",
       "      <td>176.781107</td>\n",
       "      <td>feel</td>\n",
       "      <td>45.684078</td>\n",
       "      <td>say</td>\n",
       "      <td>121.592520</td>\n",
       "      <td>say</td>\n",
       "      <td>366.013818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>make</td>\n",
       "      <td>176.191530</td>\n",
       "      <td>come</td>\n",
       "      <td>43.584496</td>\n",
       "      <td>make</td>\n",
       "      <td>115.585436</td>\n",
       "      <td>make</td>\n",
       "      <td>343.425022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>let</td>\n",
       "      <td>168.557124</td>\n",
       "      <td>let</td>\n",
       "      <td>43.300451</td>\n",
       "      <td>time</td>\n",
       "      <td>115.160339</td>\n",
       "      <td>let</td>\n",
       "      <td>337.720165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>come</td>\n",
       "      <td>167.525657</td>\n",
       "      <td>time</td>\n",
       "      <td>41.203833</td>\n",
       "      <td>come</td>\n",
       "      <td>114.138575</td>\n",
       "      <td>time</td>\n",
       "      <td>329.005521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fuck</td>\n",
       "      <td>159.747776</td>\n",
       "      <td>say</td>\n",
       "      <td>40.556409</td>\n",
       "      <td>way</td>\n",
       "      <td>103.444544</td>\n",
       "      <td>baby</td>\n",
       "      <td>328.813266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tell</td>\n",
       "      <td>154.620452</td>\n",
       "      <td>make</td>\n",
       "      <td>39.818550</td>\n",
       "      <td>heart</td>\n",
       "      <td>98.592403</td>\n",
       "      <td>come</td>\n",
       "      <td>326.532883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>shit</td>\n",
       "      <td>153.629000</td>\n",
       "      <td>want</td>\n",
       "      <td>38.281078</td>\n",
       "      <td>let</td>\n",
       "      <td>94.906574</td>\n",
       "      <td>feel</td>\n",
       "      <td>322.736016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>want</td>\n",
       "      <td>152.119749</td>\n",
       "      <td>baby</td>\n",
       "      <td>37.778148</td>\n",
       "      <td>day</td>\n",
       "      <td>91.992747</td>\n",
       "      <td>want</td>\n",
       "      <td>311.321363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>time</td>\n",
       "      <td>150.382568</td>\n",
       "      <td>tell</td>\n",
       "      <td>36.087018</td>\n",
       "      <td>baby</td>\n",
       "      <td>89.559823</td>\n",
       "      <td>heart</td>\n",
       "      <td>286.550107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>baby</td>\n",
       "      <td>144.628390</td>\n",
       "      <td>need</td>\n",
       "      <td>33.462422</td>\n",
       "      <td>night</td>\n",
       "      <td>89.139788</td>\n",
       "      <td>way</td>\n",
       "      <td>285.235689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nigga</td>\n",
       "      <td>140.372793</td>\n",
       "      <td>think</td>\n",
       "      <td>33.265863</td>\n",
       "      <td>think</td>\n",
       "      <td>89.137512</td>\n",
       "      <td>need</td>\n",
       "      <td>274.844157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>need</td>\n",
       "      <td>138.896193</td>\n",
       "      <td>way</td>\n",
       "      <td>32.667638</td>\n",
       "      <td>good</td>\n",
       "      <td>88.734402</td>\n",
       "      <td>tell</td>\n",
       "      <td>271.120429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>girl</td>\n",
       "      <td>137.077448</td>\n",
       "      <td>life</td>\n",
       "      <td>31.937362</td>\n",
       "      <td>little</td>\n",
       "      <td>88.138042</td>\n",
       "      <td>think</td>\n",
       "      <td>265.721139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>feel</td>\n",
       "      <td>135.065661</td>\n",
       "      <td>look</td>\n",
       "      <td>30.012580</td>\n",
       "      <td>tell</td>\n",
       "      <td>88.076418</td>\n",
       "      <td>life</td>\n",
       "      <td>251.117606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>way</td>\n",
       "      <td>131.865452</td>\n",
       "      <td>day</td>\n",
       "      <td>29.658697</td>\n",
       "      <td>life</td>\n",
       "      <td>87.077387</td>\n",
       "      <td>day</td>\n",
       "      <td>243.161276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>think</td>\n",
       "      <td>129.986655</td>\n",
       "      <td>night</td>\n",
       "      <td>29.521743</td>\n",
       "      <td>want</td>\n",
       "      <td>86.570361</td>\n",
       "      <td>look</td>\n",
       "      <td>230.066087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>right</td>\n",
       "      <td>128.476981</td>\n",
       "      <td>heart</td>\n",
       "      <td>29.364206</td>\n",
       "      <td>old</td>\n",
       "      <td>83.568123</td>\n",
       "      <td>away</td>\n",
       "      <td>229.556388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>man</td>\n",
       "      <td>125.763916</td>\n",
       "      <td>right</td>\n",
       "      <td>28.131436</td>\n",
       "      <td>man</td>\n",
       "      <td>83.099101</td>\n",
       "      <td>right</td>\n",
       "      <td>228.657478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hip-Hop             Electronic            Country                Pop  \\\n",
       "     ngram       tfidf      ngram      tfidf   ngram       tfidf  ngram   \n",
       "0     just  201.531755       love  62.770534    love  171.404108   love   \n",
       "1     love  183.391465       know  57.876466    know  145.763925   know   \n",
       "2      say  180.236696       just  46.493119    just  143.931513   just   \n",
       "3    bitch  176.781107       feel  45.684078     say  121.592520    say   \n",
       "4     make  176.191530       come  43.584496    make  115.585436   make   \n",
       "5      let  168.557124        let  43.300451    time  115.160339    let   \n",
       "6     come  167.525657       time  41.203833    come  114.138575   time   \n",
       "7     fuck  159.747776        say  40.556409     way  103.444544   baby   \n",
       "8     tell  154.620452       make  39.818550   heart   98.592403   come   \n",
       "9     shit  153.629000       want  38.281078     let   94.906574   feel   \n",
       "10    want  152.119749       baby  37.778148     day   91.992747   want   \n",
       "11    time  150.382568       tell  36.087018    baby   89.559823  heart   \n",
       "12    baby  144.628390       need  33.462422   night   89.139788    way   \n",
       "13   nigga  140.372793      think  33.265863   think   89.137512   need   \n",
       "14    need  138.896193        way  32.667638    good   88.734402   tell   \n",
       "15    girl  137.077448       life  31.937362  little   88.138042  think   \n",
       "16    feel  135.065661       look  30.012580    tell   88.076418   life   \n",
       "17     way  131.865452        day  29.658697    life   87.077387    day   \n",
       "18   think  129.986655      night  29.521743    want   86.570361   look   \n",
       "19   right  128.476981      heart  29.364206     old   83.568123   away   \n",
       "20     man  125.763916      right  28.131436     man   83.099101  right   \n",
       "\n",
       "                \n",
       "         tfidf  \n",
       "0   504.436677  \n",
       "1   472.331997  \n",
       "2   392.819332  \n",
       "3   366.013818  \n",
       "4   343.425022  \n",
       "5   337.720165  \n",
       "6   329.005521  \n",
       "7   328.813266  \n",
       "8   326.532883  \n",
       "9   322.736016  \n",
       "10  311.321363  \n",
       "11  286.550107  \n",
       "12  285.235689  \n",
       "13  274.844157  \n",
       "14  271.120429  \n",
       "15  265.721139  \n",
       "16  251.117606  \n",
       "17  243.161276  \n",
       "18  230.066087  \n",
       "19  229.556388  \n",
       "20  228.657478  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_tfidf.tfidf_by_genre(genres=[\"Hip-Hop\", \"Electronic\", \"Country\", \"Pop\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmas with POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TFIDF vectorizer for all genres ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5712/5712 [00:00<00:00, 124915.22it/s]\n",
      "100%|██████████| 1361/1361 [00:00<00:00, 190592.89it/s]\n",
      "100%|██████████| 3666/3666 [00:00<00:00, 257086.08it/s]\n",
      "100%|██████████| 9796/9796 [00:00<00:00, 194832.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Hip-Hop</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Electronic</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Country</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Pop</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>ngram</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>ngram</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>ngram</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love_noun</td>\n",
       "      <td>123.828691</td>\n",
       "      <td>want_verb</td>\n",
       "      <td>34.466838</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>89.520494</td>\n",
       "      <td>need_verb</td>\n",
       "      <td>241.798611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why_adv</td>\n",
       "      <td>98.466808</td>\n",
       "      <td>never_adv</td>\n",
       "      <td>33.841482</td>\n",
       "      <td>heart_noun</td>\n",
       "      <td>88.890043</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>240.074769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gon_verb</td>\n",
       "      <td>94.076031</td>\n",
       "      <td>baby_noun</td>\n",
       "      <td>33.450649</td>\n",
       "      <td>let_verb</td>\n",
       "      <td>85.696241</td>\n",
       "      <td>give_verb</td>\n",
       "      <td>233.417344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>right_adv</td>\n",
       "      <td>93.671018</td>\n",
       "      <td>love_verb</td>\n",
       "      <td>32.395608</td>\n",
       "      <td>night_noun</td>\n",
       "      <td>81.134652</td>\n",
       "      <td>life_noun</td>\n",
       "      <td>228.985391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ass_noun</td>\n",
       "      <td>89.826484</td>\n",
       "      <td>tell_verb</td>\n",
       "      <td>32.369741</td>\n",
       "      <td>could_verb</td>\n",
       "      <td>80.768520</td>\n",
       "      <td>here_adv</td>\n",
       "      <td>223.220594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>real_adj</td>\n",
       "      <td>88.399610</td>\n",
       "      <td>think_verb</td>\n",
       "      <td>29.710738</td>\n",
       "      <td>think_verb</td>\n",
       "      <td>80.486791</td>\n",
       "      <td>day_noun</td>\n",
       "      <td>221.498472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>night_noun</td>\n",
       "      <td>87.133135</td>\n",
       "      <td>need_verb</td>\n",
       "      <td>29.000614</td>\n",
       "      <td>little_adj</td>\n",
       "      <td>80.035263</td>\n",
       "      <td>away_adv</td>\n",
       "      <td>209.293946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hear_verb</td>\n",
       "      <td>85.884731</td>\n",
       "      <td>way_noun</td>\n",
       "      <td>28.927436</td>\n",
       "      <td>tell_verb</td>\n",
       "      <td>79.957868</td>\n",
       "      <td>thing_noun</td>\n",
       "      <td>208.280183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>run_verb</td>\n",
       "      <td>85.314507</td>\n",
       "      <td>give_verb</td>\n",
       "      <td>28.877181</td>\n",
       "      <td>baby_noun</td>\n",
       "      <td>79.579543</td>\n",
       "      <td>night_noun</td>\n",
       "      <td>205.457199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>really_adv</td>\n",
       "      <td>85.150709</td>\n",
       "      <td>would_verb</td>\n",
       "      <td>28.721585</td>\n",
       "      <td>good_adj</td>\n",
       "      <td>79.433910</td>\n",
       "      <td>girl_noun</td>\n",
       "      <td>202.812230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>one_noun</td>\n",
       "      <td>84.084184</td>\n",
       "      <td>life_noun</td>\n",
       "      <td>28.628776</td>\n",
       "      <td>life_noun</td>\n",
       "      <td>79.177233</td>\n",
       "      <td>look_verb</td>\n",
       "      <td>200.488828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>heart_noun</td>\n",
       "      <td>82.297298</td>\n",
       "      <td>be_verb</td>\n",
       "      <td>28.173637</td>\n",
       "      <td>how_adv</td>\n",
       "      <td>78.013572</td>\n",
       "      <td>good_adj</td>\n",
       "      <td>200.024706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>like_verb</td>\n",
       "      <td>82.107461</td>\n",
       "      <td>how_adv</td>\n",
       "      <td>27.626259</td>\n",
       "      <td>where_adv</td>\n",
       "      <td>77.922929</td>\n",
       "      <td>keep_verb</td>\n",
       "      <td>199.757546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>big_adj</td>\n",
       "      <td>81.432636</td>\n",
       "      <td>here_adv</td>\n",
       "      <td>27.527959</td>\n",
       "      <td>want_verb</td>\n",
       "      <td>77.906892</td>\n",
       "      <td>back_adv</td>\n",
       "      <td>198.455638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bad_adj</td>\n",
       "      <td>80.359399</td>\n",
       "      <td>where_adv</td>\n",
       "      <td>26.799125</td>\n",
       "      <td>old_adj</td>\n",
       "      <td>76.829439</td>\n",
       "      <td>find_verb</td>\n",
       "      <td>198.062386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>live_verb</td>\n",
       "      <td>79.872648</td>\n",
       "      <td>day_noun</td>\n",
       "      <td>26.755513</td>\n",
       "      <td>back_adv</td>\n",
       "      <td>76.445619</td>\n",
       "      <td>leave_verb</td>\n",
       "      <td>188.424609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>more_adj</td>\n",
       "      <td>79.677436</td>\n",
       "      <td>could_verb</td>\n",
       "      <td>26.569848</td>\n",
       "      <td>man_noun</td>\n",
       "      <td>76.379137</td>\n",
       "      <td>try_verb</td>\n",
       "      <td>187.120056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>talk_verb</td>\n",
       "      <td>79.161538</td>\n",
       "      <td>night_noun</td>\n",
       "      <td>26.322839</td>\n",
       "      <td>thing_noun</td>\n",
       "      <td>75.408729</td>\n",
       "      <td>too_adv</td>\n",
       "      <td>185.053192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>boy_noun</td>\n",
       "      <td>78.432644</td>\n",
       "      <td>heart_noun</td>\n",
       "      <td>26.286504</td>\n",
       "      <td>here_adv</td>\n",
       "      <td>74.209383</td>\n",
       "      <td>where_adv</td>\n",
       "      <td>185.052587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>niggas_noun</td>\n",
       "      <td>77.213881</td>\n",
       "      <td>find_verb</td>\n",
       "      <td>25.372413</td>\n",
       "      <td>leave_verb</td>\n",
       "      <td>73.456591</td>\n",
       "      <td>world_noun</td>\n",
       "      <td>184.512008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>play_verb</td>\n",
       "      <td>76.655721</td>\n",
       "      <td>away_adv</td>\n",
       "      <td>25.339043</td>\n",
       "      <td>feel_verb</td>\n",
       "      <td>73.418149</td>\n",
       "      <td>why_adv</td>\n",
       "      <td>182.901344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Hip-Hop              Electronic                Country             \\\n",
       "          ngram       tfidf       ngram      tfidf       ngram      tfidf   \n",
       "0     love_noun  123.828691   want_verb  34.466838     be_verb  89.520494   \n",
       "1       why_adv   98.466808   never_adv  33.841482  heart_noun  88.890043   \n",
       "2      gon_verb   94.076031   baby_noun  33.450649    let_verb  85.696241   \n",
       "3     right_adv   93.671018   love_verb  32.395608  night_noun  81.134652   \n",
       "4      ass_noun   89.826484   tell_verb  32.369741  could_verb  80.768520   \n",
       "5      real_adj   88.399610  think_verb  29.710738  think_verb  80.486791   \n",
       "6    night_noun   87.133135   need_verb  29.000614  little_adj  80.035263   \n",
       "7     hear_verb   85.884731    way_noun  28.927436   tell_verb  79.957868   \n",
       "8      run_verb   85.314507   give_verb  28.877181   baby_noun  79.579543   \n",
       "9    really_adv   85.150709  would_verb  28.721585    good_adj  79.433910   \n",
       "10     one_noun   84.084184   life_noun  28.628776   life_noun  79.177233   \n",
       "11   heart_noun   82.297298     be_verb  28.173637     how_adv  78.013572   \n",
       "12    like_verb   82.107461     how_adv  27.626259   where_adv  77.922929   \n",
       "13      big_adj   81.432636    here_adv  27.527959   want_verb  77.906892   \n",
       "14      bad_adj   80.359399   where_adv  26.799125     old_adj  76.829439   \n",
       "15    live_verb   79.872648    day_noun  26.755513    back_adv  76.445619   \n",
       "16     more_adj   79.677436  could_verb  26.569848    man_noun  76.379137   \n",
       "17    talk_verb   79.161538  night_noun  26.322839  thing_noun  75.408729   \n",
       "18     boy_noun   78.432644  heart_noun  26.286504    here_adv  74.209383   \n",
       "19  niggas_noun   77.213881   find_verb  25.372413  leave_verb  73.456591   \n",
       "20    play_verb   76.655721    away_adv  25.339043   feel_verb  73.418149   \n",
       "\n",
       "           Pop              \n",
       "         ngram       tfidf  \n",
       "0    need_verb  241.798611  \n",
       "1      be_verb  240.074769  \n",
       "2    give_verb  233.417344  \n",
       "3    life_noun  228.985391  \n",
       "4     here_adv  223.220594  \n",
       "5     day_noun  221.498472  \n",
       "6     away_adv  209.293946  \n",
       "7   thing_noun  208.280183  \n",
       "8   night_noun  205.457199  \n",
       "9    girl_noun  202.812230  \n",
       "10   look_verb  200.488828  \n",
       "11    good_adj  200.024706  \n",
       "12   keep_verb  199.757546  \n",
       "13    back_adv  198.455638  \n",
       "14   find_verb  198.062386  \n",
       "15  leave_verb  188.424609  \n",
       "16    try_verb  187.120056  \n",
       "17     too_adv  185.053192  \n",
       "18   where_adv  185.052587  \n",
       "19  world_noun  184.512008  \n",
       "20     why_adv  182.901344  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_tfidf.preprocessed_corpus = tagged_corpus\n",
    "lyrics_tfidf.tfidf_by_genre(genres=[\"Hip-Hop\", \"Electronic\", \"Country\", \"Pop\"], doc_frequency=(0.001, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Latent Topics from Matrix Factorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, H = lyrics_tfidf.matrix_factorization(method='nmf', n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-78e376611dd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlyrics_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_latent_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-4fc21f733c2d>\u001b[0m in \u001b[0;36mprint_latent_topics\u001b[0;34m(self, lower_dimensional_words, vocabulary, topn)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{i+1}:\\t'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtopic\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-4fc21f733c2d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{i+1}:\\t'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtopic\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "lyrics_tfidf.print_latent_topics(H, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
