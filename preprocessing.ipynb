{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm # progress bar when long task\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import Counter\n",
    "import spacy\n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LyricsPreprocessing CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class LyricsPreprocessing():\n",
    "    \n",
    "    \n",
    "    def __init__(self, lyrics_df):\n",
    "        \n",
    "        self.lyrics_df = lyrics_df\n",
    "        \n",
    "        \n",
    "    def preliminary_preprocessing(self, lyrics_df=None, replace_number_with=\"0\", every_digit=True):\n",
    "        \n",
    "        if lyrics_df is None:\n",
    "            lyrics_df = self.lyrics_df\n",
    "        \n",
    "        # lowercase\n",
    "        print(\"Converting to lower case ...\", flush=True)\n",
    "        lyrics_df.loc[:, \"lyrics\"] = lyrics_df.lyrics.apply(lambda text: text.strip().lower())\n",
    "        \n",
    "        # replace numbers\n",
    "        print(f\"Replacing numbers with {replace_number_with} ...\", flush=True)\n",
    "        if every_digit:\n",
    "            numbers = re.compile('[0-9]+')\n",
    "        else:\n",
    "            numbers = re.compile('[0-9]')\n",
    "        \n",
    "        lyrics_df.loc[:, \"lyrics\"] = lyrics_df.lyrics.apply(lambda text: re.sub(numbers, replace_number_with, text))\n",
    "        \n",
    "        return lyrics_df\n",
    "\n",
    "    \n",
    "    def tokenize(self, lyrics_df=None, save=None):\n",
    "        \n",
    "        if lyrics_df is None:\n",
    "            lyrics_df = self.lyrics_df\n",
    "        \n",
    "        print(\"Tokenizing ...\", flush=True)\n",
    "        tokenized_corpus = [[token.text for token in nlp(lyrics)] for lyrics in tqdm(lyrics_df.loc[:, \"lyrics\"].to_list())]\n",
    "        self.tokenized_corpus = tokenized_corpus\n",
    "        \n",
    "        if not save is None:\n",
    "            self.save(tokenized_corpus, 'tokenized_corpus', how=save)\n",
    "        \n",
    "        return tokenized_corpus\n",
    "    \n",
    "    \n",
    "    def core_preprocessing(self, lyrics_df, corpus):\n",
    "        \n",
    "        if lyrics_df is None:\n",
    "            lyrics_df = self.lyrics_df\n",
    "            \n",
    "        trimmed_corpus = []\n",
    "        tagged_corpus = []\n",
    "        for lyrics in tqdm(lyrics_df.loc[:, \"lyrics\"].to_list()):\n",
    "            trimmed_lemmas = []\n",
    "            tagged_lemmas = []\n",
    "            for token in nlp(lyrics):\n",
    "                # remove all tokens with length <= 2\n",
    "                if len(str(token)) > 2:\n",
    "                    # lemmatize and get POS\n",
    "                    lemma = token.lemma_\n",
    "                    pos = token.pos_\n",
    "                    # keep only if POS in ['NOUN', 'VERB', 'ADJ', 'ADV','INTJ', 'X']\n",
    "                    if pos in ['NOUN', 'VERB', 'ADJ', 'ADV', 'INTJ', 'X']:\n",
    "                        trimmed_lemmas.append(lemma)\n",
    "                        tagged_lemmas.append(lemma + \"_\" + pos)\n",
    "            trimmed_corpus.append(trimmed_lemmas)\n",
    "            tagged_corpus.append(tagged_lemmas)\n",
    "        \n",
    "        self.trimmed_corpus = trimmed_corpus\n",
    "        self.tagged_corpus = tagged_corpus\n",
    "        \n",
    "           \n",
    "    def join_collocations(self, preprocessed_corpus, most_common=20):\n",
    "        \n",
    "        # separate each lyrics with a space in order to avoid collocations between different songs\n",
    "        words = []\n",
    "        for idx, lyrics in enumerate(preprocessed_corpus):\n",
    "            # not for the first\n",
    "            if idx > 0:\n",
    "                words.append(' ')\n",
    "            for token in lyrics:\n",
    "                words.append(token)\n",
    "            \n",
    "        finder = BigramCollocationFinder.from_words(words)\n",
    "        bgm = BigramAssocMeasures()\n",
    "        score = bgm.mi_like\n",
    "        collocations = {'_'.join(bigram): pmi for bigram, pmi in finder.score_ngrams(score)}\n",
    "        \n",
    "        return Counter(collocations).most_common(most_common)\n",
    "    \n",
    "    \n",
    "    def save(self, obj, filename, path='/Users/lucamasserano/Desktop/BOCCONI/nlp/final_project/lyrics_project/data/', how='pickle'):\n",
    "        \n",
    "        if how == 'pickle':\n",
    "            with open(os.path.join(path, filename+'.pickle'), 'rb') as pickled_obj:\n",
    "                pickle.dump(obj, pickled_object)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_preprocess = LyricsPreprocessing(lyrics_df=lyrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to lower case ...\n",
      "Replacing numbers with  ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "artist                                          Bruce Springsteen\n",
       "song                                         Racing In The Street\n",
       "album                                Darkness on the Edge of Town\n",
       "release_date                                           1978-06-02\n",
       "genre                                                        Rock\n",
       "lyrics          i got a sixty-nine chevy with a 0 fuelie heads...\n",
       "year                                                         1978\n",
       "Name: 4758, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_cleaned = lyrics_preprocess.preliminary_preprocessing()\n",
    "lyrics_cleaned.iloc[4758, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization took some time to finish. Let's pickle it to make it easily reusable\n",
    "with open('./data/trimmed_corpus.pickle', 'rb') as pickled_object:\n",
    "    trimmed_corpus = pickle.load(pickled_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_idxs = lyrics_df.loc[lyrics_df.genre == \"Hip-Hop\", :].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('la_la', 428.0013218590605),\n",
       " ('ooh_ooh', 157.50899525998275),\n",
       " ('hip_hop', 59.791212630751424),\n",
       " ('zoote_zoote', 54.90012558662172),\n",
       " ('da_da', 49.29725050967009),\n",
       " ('no_matter', 38.71592661934151),\n",
       " ('o_o', 38.685505609873736),\n",
       " ('boom_boom', 37.862219323102245),\n",
       " ('unfazed_unfazed', 37.5741965973535),\n",
       " ('oo_oo', 37.03362978638222),\n",
       " ('tick_tock', 32.255857966834895),\n",
       " ('at_all', 28.039869258449368),\n",
       " ('ling_ling', 27.428571428571427),\n",
       " ('icky_icky', 27.34375),\n",
       " ('bye_bye', 26.885108170552407),\n",
       " ('brand_new', 24.787714543812104),\n",
       " ('sensual_seduction', 23.335714285714285),\n",
       " ('diddly_diddly', 23.03810650887574),\n",
       " ('motivated_motivated', 22.345270890725438),\n",
       " ('doo_doo', 22.07128460091373),\n",
       " ('too_much', 21.809510647908752),\n",
       " ('thicky_thicky', 20.444444444444443),\n",
       " ('•_•', 20.301854191438764),\n",
       " ('doot_doot', 20.195763330898465),\n",
       " ('win_win', 19.419370153701983),\n",
       " ('minor_setback', 18.889068825910933),\n",
       " ('lovey_dovey', 18.776234567901234),\n",
       " ('knooow_beee', 18.0),\n",
       " ('heya_heya', 17.99852071005917),\n",
       " ('wishy_washy', 17.925925925925927),\n",
       " ('no_more', 17.264767391729475),\n",
       " ('little_bit', 17.21578616415988),\n",
       " ('right_now', 16.887962972010943),\n",
       " ('scarface_scarface', 16.669020371723075),\n",
       " ('kool_aid', 16.129032258064516),\n",
       " ('ad_libs', 14.58),\n",
       " ('would_rather', 13.934989274253432),\n",
       " ('re_-', 13.845914758574613),\n",
       " ('let_go', 13.611150484541724),\n",
       " ('0rd_0rd', 13.529583637691745),\n",
       " ('slatt_slatt', 13.06616257088847),\n",
       " ('dit_dit', 12.965973534971644),\n",
       " ('wake_wake', 12.843069897578784),\n",
       " ('whoa_whoa', 12.660770031217481),\n",
       " ('tiding_tiding', 12.597623966942148),\n",
       " ('so_much', 12.323870962041543),\n",
       " ('skrrt_skrrt', 12.265460300397145),\n",
       " ('ice_cube', 10.766095411728958),\n",
       " ('bank_account', 10.634908494946735),\n",
       " ('looky_looky', 10.448979591836734),\n",
       " ('eah_eah', 10.380622837370241),\n",
       " ('f**k_f**k', 10.380622837370241),\n",
       " ('dab_ranch', 10.378842895182764),\n",
       " ('clickity_clank', 10.285714285714286),\n",
       " ('eyed_pea', 10.126358053849787),\n",
       " (\"er'uoy_tuo\", 10.0),\n",
       " (\"ruoy_er'uoy\", 10.0),\n",
       " ('love_love', 9.957071549202727),\n",
       " ('range_rover', 9.93880274237335),\n",
       " ('panda_panda', 9.777777777777779),\n",
       " ('salt_shaker', 9.506651884700664),\n",
       " ('minimum_wage', 9.217424242424242),\n",
       " ('swagger_swagger', 8.8664453125),\n",
       " ('mira_mira', 8.816326530612244),\n",
       " ('check_check', 8.741096803457253),\n",
       " ('p_p', 8.72792731279242),\n",
       " ('hitta_hitta', 8.610683698650021),\n",
       " ('low_low', 8.587395578156116),\n",
       " ('beee_knooow', 8.469135802469136),\n",
       " ('sizzz_ore', 8.333333333333334),\n",
       " ('withhold_withhold', 8.324661810613943),\n",
       " ('rest_peace', 8.172627845107892),\n",
       " ('bicken_bool', 8.165476190476191),\n",
       " (\"boo'd_biddy\", 8.152133580705009),\n",
       " ('ele_vator', 8.1),\n",
       " ('gang_gang', 8.004005056169774),\n",
       " ('dawgs_dawgs', 8.0),\n",
       " ('wicked_wicked', 7.849171597633136),\n",
       " ('secure_secure', 7.837098692033294),\n",
       " ('fi_fi', 7.836734693877551),\n",
       " ('thu_thu', 7.6011080332409975),\n",
       " ('puerto_rican', 7.5),\n",
       " ('bark_bark', 7.475725044063451),\n",
       " ('shake_shake', 7.365463857426013),\n",
       " ('knick_knack', 7.305194805194805),\n",
       " ('defari_herut', 7.29),\n",
       " ('click_clack', 7.285974499089253),\n",
       " ('sic_youngin', 7.259193619849357),\n",
       " ('0/0/0_•', 7.202179118773946),\n",
       " ('fantastic_voyage', 7.187780772686433),\n",
       " ('give_fuck', 7.121536030710783),\n",
       " ('outpouring_unleash', 7.062068965517241),\n",
       " ('know_know', 7.043360990250717),\n",
       " ('know_how', 7.002386900978009),\n",
       " ('biggie_biggie', 6.674293950722941),\n",
       " ('wining_dining', 6.666666666666667),\n",
       " ('too_many', 6.657473472566264),\n",
       " ('incredible_incredible', 6.648179769141452),\n",
       " ('saint_laurent', 6.545454545454546),\n",
       " ('tremble_vibrator', 6.533333333333333)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_preprocess.join_collocations(np.array(trimmed_corpus)[genre_idxs].tolist(), most_common=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATISTICS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
